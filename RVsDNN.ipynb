{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XfwiLMK_5J4u"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3459be2f1dcf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstrftime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from time import strftime\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QY7YdvJs5WAE",
    "outputId": "7796a540-9af8-4352-9567-016610f80786"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive not mounted, so nothing to flush and unmount.\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from google.colab import drive\n",
    "drive.flush_and_unmount()\n",
    "drive.mount(os.path.normpath(r'/content/drive/'), force_remount=True)\n",
    "sys.path.append('/content/drive/My Drive/PythonScripts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PjDeEkf9Dg_y",
    "outputId": "eb4cb3ac-2f10-40b6-9394-5e97291184e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'make_RVs' from '/content/drive/My Drive/PythonScripts/make_RVs.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sys.setrecursionlimit(100)  # Increase the maximum recursion depth to 10000\n",
    "import importlib\n",
    "\n",
    "import make_RVs\n",
    "importlib.reload(make_RVs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yLz-BKmYDZDw"
   },
   "outputs": [],
   "source": [
    "T = \"T0\"\n",
    "N_OF_PS = \"NumOfPeriods\"\n",
    "PERIOD = \"Period\"\n",
    "ECC = \"Eccentricity\"\n",
    "OMEGA = \"OMEGA\"\n",
    "K1_STR = \"K1\"\n",
    "K2_STR = \"K2\"\n",
    "GAMMA = \"GAMMA\"\n",
    "RANGE = \"Range\"\n",
    "SAMPLES = \"Samples\"\n",
    "RVS = \"RadialVelocities\"\n",
    "ERR_RVS = \"RVErrors\"\n",
    "TS = \"TimeStamps\"\n",
    "TS_DIFF = TS + \"Diff\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "bj8bp_n05aJK",
    "outputId": "02cb3e4e-4ae1-46fd-b5d4-2d09e30caedc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [12:36<00:00, 1321.61it/s]\n"
     ]
    }
   ],
   "source": [
    "#data creation\n",
    "T0_RANGE = (0, 1)\n",
    "N_OF_PS_RANGE = (0, 6)\n",
    "ECC_RANGE = (0, 1)\n",
    "OMEGA_RANGE = (0, np.pi)\n",
    "K1_RANGE = (0, 0)\n",
    "K2_RANGE = (0, 0)\n",
    "GAMMA_RANGE = (0, 50)\n",
    "NRV = 25\n",
    "sig_RV = 3.\n",
    "ARGS_DICT = {T: {RANGE: T0_RANGE},\n",
    "              ECC: {RANGE: ECC_RANGE},\n",
    "              OMEGA: {RANGE: OMEGA_RANGE},\n",
    "              K1_STR: {RANGE: K1_RANGE},\n",
    "              K2_STR: {RANGE: K2_RANGE},\n",
    "              GAMMA: {RANGE: GAMMA_RANGE},\n",
    "              N_OF_PS: {RANGE:N_OF_PS_RANGE},\n",
    "}\n",
    "N_OF_SAMPS = int(1e6)\n",
    "\n",
    "# timestr = strftime(\"%Y%m%d-%H%M%S\") + \"_Trues/\"\n",
    "# timestr = \"/Falses/\"\n",
    "timestr = \"/Falses/\"\n",
    "OUTDIR = os.path.normpath(r\"/content/drive/My Drive/PythonScripts/scriptsOut/RVDataGen/{}/\".format(timestr))\n",
    "os.makedirs(OUTDIR,exist_ok=True)\n",
    "out_fp_format = OUTDIR+ r\"/{}.parquet\"\n",
    "make_RVs.out_multiple_and_dump(ARGS_DICT, NRV, sig_RV, N_OF_SAMPS, out_fp_format,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "id": "jBlM0SAudLM8"
   },
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",q
    "    def __init__(self, true_dir, false_dir):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Load true data\n",
    "        for filename in os.listdir(true_dir):\n",
    "            file_path = os.path.join(true_dir, filename)\n",
    "            if file_path.endswith('.parquet'):\n",
    "                df = pd.read_parquet(file_path)\n",
    "                self.data.append(df.features)  # Assuming df is structured correctly\n",
    "\n",
    "                df['cat'] = df['labels']\n",
    "                self.labels.append(df.cat)  # True label\n",
    "\n",
    "        # Load false data\n",
    "        for filename in os.listdir(false_dir):\n",
    "            file_path = os.path.join(false_dir, filename)\n",
    "            if file_path.endswith('.parquet'):\n",
    "                df = pd.read_parquet(file_path)\n",
    "                self.data.append(df.features)\n",
    "\n",
    "                df['cat'] = df['labels']\n",
    "                self.labels.append(df.cat)  # False label\n",
    "\n",
    "        # Convert to numpy arrays for processing\n",
    "        # print(self.data)\n",
    "\n",
    "        self.data = np.stack(np.array(pd.concat(self.data, axis=0)), axis = 0)\n",
    "        self.labels = np.stack(np.array(pd.concat(self.labels, axis=0)), axis=0).reshape(-1,1)\n",
    "        # print(\"\\n\\n\" , self.data)\n",
    "\n",
    "        # Normalize data\n",
    "        scaler = StandardScaler()\n",
    "        self.data = scaler.fit_transform(self.data.reshape(-1, self.data.shape[-1])).reshape(self.data.shape)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        self.data = torch.tensor(self.data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Example usage\n",
    "true_dir = '/content/drive/My Drive/PythonScripts/scriptsOut/RVDataGen/Trues/'\n",
    "false_dir = '/content/drive/My Drive/PythonScripts/scriptsOut/RVDataGen/Falses/'\n",
    "dataset = CSVDataset(true_dir, false_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Iterating through the DataLoader\n",
    "# for features, labels in dataloader:\n",
    "    # print(features, labels)  # features are the input tensors, labels are the target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jfO2GOJxaZQx",
    "outputId": "7633b996-7aa7-4342-9382-c71589d371c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1987786, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4g3S0111kqZB",
    "outputId": "91bf79fe-0b80-43b5-8319-69d78557e19d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# Initialize the SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sd4wqf0qM_O1",
    "outputId": "dd94bc3b-1fe8-45e8-e195-af7288933acc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "NGAGpgVD48De",
    "outputId": "87bdaca4-e3e9-44a2-bc5d-cb20e27c7490"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-db054d6f8a61>:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([1590228, 99]), y_train shape: torch.Size([1590228, 1])\n",
      "X_test shape: torch.Size([397558, 99]), y_test shape: torch.Size([397558, 1])\n",
      "Epoch 1, Loss: 0.11006039705176954\n",
      "Epoch 1, Test Accuracy: 0.9769\n",
      "Epoch 2, Loss: 0.07522690570784413\n",
      "Epoch 2, Test Accuracy: 0.9809\n",
      "Epoch 3, Loss: 0.06451682461041383\n",
      "Epoch 3, Test Accuracy: 0.9854\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class MultiOutputNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiOutputNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(99, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.output(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        loss = torch.mean((outputs - targets) ** 2)\n",
    "        return loss\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=25):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                preds = (outputs > 0.5).float()\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        print(f'Epoch {epoch + 1}, Test Accuracy: {accuracy:.4f}')\n",
    "        model.train()\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "# Example dataset\n",
    "X = dataset.data\n",
    "y = dataset.labels\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Ensure y_train and y_test are the correct shape\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = MultiOutputNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2T7TqyY_NjTm"
   },
   "outputs": [],
   "source": [
    "X = dataset.data\n",
    "y = dataset.labels\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D4EALrMONrbR",
    "outputId": "70c550a4-111c-4c94-c116-6b00b1052365"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1590280"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "SblvdEMJ4cqd",
    "outputId": "db21a106-66ac-4c35-c1f5-7d4bb9318d41"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (8) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-bc581f80572e>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Evaluate the model using the same or different dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Accuracy: {accuracy}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-bc581f80572e>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (8) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total = correct = 0\n",
    "\n",
    "    with torch.no_grad():  # Inference without gradient calculation\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the model using the same or different dataloader\n",
    "accuracy = evaluate_model(model, dataloader)\n",
    "print(f'Accuracy: {accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2v6C-l_Hdt6Z",
    "outputId": "ddb2cbe1-91f3-40c7-d3a3-90648405cd38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.8355, -1.8628, -0.7972, -1.2789,  0.7267,  2.5410, -0.1281, -3.3169],\n",
      "        [ 0.1385,  0.1458,  1.2947, -2.1771,  0.9538,  2.0980,  2.0213, -3.1400],\n",
      "        [-0.1988, -0.2840,  1.0944, -1.7603,  1.2204,  1.8472,  1.8446, -3.3373],\n",
      "        [-3.3031, -3.1660, -2.0605,  0.3435,  1.2621,  1.2703, -1.4575, -2.6739],\n",
      "        [-3.7641, -4.0797, -2.7013,  0.9149,  1.2173,  0.5876, -1.9664, -3.1593],\n",
      "        [-2.0217, -1.9928, -0.8425, -1.2127,  0.5669,  2.5521, -0.2204, -3.6047],\n",
      "        [-3.3311, -3.5428, -2.3368,  1.0954,  1.1001, -0.1368, -1.7017, -2.7763],\n",
      "        [-3.1594, -3.3270, -1.9895,  1.5858,  1.5150, -1.0384, -1.4082, -2.4099],\n",
      "        [-3.0317, -3.3122, -2.2047,  0.8855,  1.0915,  0.0382, -1.5674, -2.6818],\n",
      "        [-3.3979, -3.3809, -2.3284,  0.8726,  1.1729,  0.4294, -1.6530, -2.7558]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "torch.return_types.max(\n",
      "values=tensor([2.5410, 2.0980, 1.8472, 1.2703, 1.2173, 2.5521, 1.1001, 1.5858, 1.0915,\n",
      "        1.1729]),\n",
      "indices=tensor([5, 5, 5, 5, 4, 5, 4, 3, 4, 4]))\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "for inputs, labels in dataloader:\n",
    "  outputs = model(inputs)\n",
    "  print(outputs)\n",
    "  print(torch.max(outputs.data, 1))\n",
    "  print( labels.size(0))\n",
    "  break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5Leogbk5FUT"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rvENV",
   "language": "python",
   "name": "rvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
